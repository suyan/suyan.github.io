[TOC]

# Introduction

This is intended as a simple summary for me to review Deep Learning models for NLP task that I learned these days.
In the end of this post, I constructed an attention based LSTM to process sentiment analysis as a practice .
There are still some problems exists, I am working on it now.
1. Attention part was not finished
2. Embedding was not good enough, I am trying to construct an emotional word representation approach.
## My Interests in NLP
1. Is embedding good enough to represent semantic? can we find other approaches to replace embedding? I don't that embedding is not good enough, at least it can be optomized.
2. Does RNN includes biological meanings? Can we construct biology neural system based algorithom for NLP just like convolution for image processing?
3. Trainning data are  small in size, furthermore, data form is too simple for language processing. Is there any possible way that we can take image even video as input?  Language understanding is more than words.
4. Unsupervised learning is a trend of machine learning, especially in NLP field.  In most case, NLP tasks have no specific answer, it will be better to let models learn things by themself. It can also relieve  data pressure.
## NLP Tasks
Normally, there are two main components in NLP field.

1. Natural Language Understanding (NLU)
	* relation extraction
	* sentiment analysis
	* summarization
	* ...


2. Natural Language Generation (NLG)
	* machine translation
	* Question Answering
	* ...

## Machine learning and Deep Learning in NLP
Nature Language Processing is about teaching computer to process human language.

Traditional NLP systems typically rely on sophisticated feature engineering based on the statistical properties of text. In short, these systems are complex, heavily relies on human-designed representations and input features, and a lot of engineering effort goes into building them. Neural network based models work a bit differently, they can extract feature automatically by mapping semantics into vector.
Since high peformance hardware and powerful deep learning algorithms are available, lots of NLP tasks are going well by implementing Deep Learning models.

# Sentiment Analysis with Deep Learning Model

## Introduction

> Step 1: Data Processing: Process original corpora.

> Step 2: Embedding: Map nature language into vectors.

> Step 3: Model: Choose appropiate model and train it.

> Step 4: Classification: use the trained model to do sentiment analysis.

## Embedding
> Embedding is a core technique in NLP tasks.

Deep Learning model requires embeddings as inputs, therefore, it is important for us to focus on mapping corpora into embeddings in an appropriate way first.

### One-hot Representation

A simple method is One-hot Representation.
For example, chart 1 shows the One-hot representation of sentence "I have a pen". 

|words | representation|
|----- |--------------|
|I|     [1 0 0 0]|
|have|  [0 1 0 0]|
|a|     [0 0 1 0]|
|pen|   [0 0 0 1]|

Chart 1

One-hot representation is easy to understand and corpora processing can be extremely convenient, however, it suffers from two weakness.

1. Vectors can be very large in size.
2. It ignores relationships between words.

### Distributed representation

#### Word to vectors
Back to 1986, Rumelhart and Hinton already introduced the earliest use of distributed representation, this idea has been proved successful in embedding problem for NLP.
Such approaches are cost-effective and computation-efficient for trainning because of low dimension of word embedding, furthermore  relationship of words reflects on their distance in the vector space.
Basic idea of word to vector is mapping words to size fixed vectors (typically 50~200 dimension, which is very short comparing to One-hot representatin). Then put all vectors together to construct a word vector space. We can see that each word becomes a point in the vector space which is extremely useful(e.g. the distance between words can be calculated which contains lots of information: near distance means close relationship).





#### Sentence to vectors
Word embedding have been proved to be successful in many NLP tasks, however , are not optimized specialaly for representing sentences.  But word to vectors  still have some wekness: they lose the ordering of the words and they also ignore semantics of the words.[8] In this case, sentence embedding is proposed for two reasons: Word embedding are usually not task-specific while sentence embedding can obtain more specific information for different tasks. Another reason is that sentence embedding  contains more complete information of a sentence.

Aiming at optimizing  word  embedding  for sentence representations. Useful sentence embedding method was proposed(Tom Kenter and Alexey Borisov 2016). They present Siamese CBOW, an efficient neural network architecture for obtaining high-quality word embedding,  directly  optimized  for  sentence representations. The Siamese CBOW provides a robust way of generating high-quality sentence.

#### Character to vectors
English sentence consists of words so approaches toward achieving embedding are often on the word level.
But Japanese and Chinese fundamental units are  Japanese or Chinese character.
Take Chinese as example, Chinese words can be divided into different Chinese character so we can get character embedding instead of word embedding. Each Chinese character has its own meaning, therefore, model works surprisingly good when use character embedding as basic input.(e.g. corpora data are very small)



## Deep Learning Models
Deep Learning based language models have been proved very powerful  comparing to traditional models such as n-gram model.

### RNNLM--Mikolov
In <<Recurrent neural network based language model>>[5], Mikolov use RNN to train language model. The biggest advantage of RNN language model is that it can make good use of history information. We can see in Figure 1, RNN can catch history information. Further more, RNN accepts variable-length input, making NLP tasks easy to process.
![RNN-overview](F:\Documents\GitHub\zhaokangkang0572.github.io\graph)
<center>RNN-overview.png<center/>

### Attention Mechanisms
With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far.[6]
# Attention Based LSTM Language Model
There are some challenges in sentiment analysis, one of them is that sentiment information can appear at any position in the sentence. To tackle these problem, I choose Attention Based LSTM Language Model to deal with a sentiment analysis task.
Given a movie review, the model attempts to predict whether it is positive or negative

## Model layers
|steps          |operation|
|---------------|---------|
|Input layer    |movie reviews from the Large Movie Review Dataset[2]|
|Embedding layer |map each sentence into a vectors|
|LSTM layer     |capture features from sentence embedding|
|Attention layer |merge word-level features from each time step|
|Output layer   |merge information from all time step by using Sigmoid from sentiment classification|

![image](F:\Documents\GitHub\zhaokangkang0572.github.io\graph\AttentionBasedLSTM_detal.png)

Figure 2 AttentionBasedLSTM_Overview
![image](F:\Documents\GitHub\zhaokangkang0572.github.io\graph\AttentionBasedLSTM_detal.png)

 Figure 3 AttentionBasedLSTM_detal

## Code

> LSTM code

```python
# -*- coding: utf-8 -*-
import numpy
import six.moves.cPickle as pickle
import numpy as np

def sigmoid(x):
    return 1. / (1 + np.exp(-x))
#Generate random arrays(-0.1~0.1)
def rand_arr(*args):
    np.random.seed(0)
    return np.random.rand(*args)*(0.2)-0.1

class LSTM_net:
    def __init__(self,x_dim,cell_dim):

        self.lstm_node_list=[]
        self.lstm_x_list=[]

        # here  input_dim is a combination of input x(step t) and output of hidden layer(step t-1)
        self.input_dim=x_dim+cell_dim
        self.x_dim=x_dim
        self.cell_dim=cell_dim

        self.param_gw=rand_arr(cell_dim,self.input_dim)
        self.param_iw=rand_arr(cell_dim,self.input_dim)
        self.param_fw=rand_arr(cell_dim,self.input_dim)
        self.param_ow=rand_arr(cell_dim,self.input_dim)
        #weight pw
        self.param_pw=rand_arr(cell_dim)

        self.param_gb=rand_arr(cell_dim)
        self.param_ib=rand_arr(cell_dim)
        self.param_fb=rand_arr(cell_dim)
        self.param_ob=rand_arr(cell_dim)

        self.param_pb=rand_arr(1)

        self.param_gw_diff=np.zeros((cell_dim,self.input_dim))
        self.param_iw_diff=np.zeros((cell_dim,self.input_dim))
        self.param_fw_diff=np.zeros((cell_dim,self.input_dim))
        self.param_ow_diff=np.zeros((cell_dim,self.input_dim))
        self.param_pw_diff=np.zeros((cell_dim))

        self.param_gb_diff=np.zeros((cell_dim))
        self.param_ib_diff=np.zeros((cell_dim))
        self.param_fb_diff=np.zeros((cell_dim))
        self.param_ob_diff=np.zeros((cell_dim))
        self.param_pb_diff=np.zeros(1)

    def apply_diff(self,lr=1):
        self.param_gw-=lr*self.param_gw_diff
        self.param_iw-=lr*self.param_iw_diff
        self.param_fw-=lr*self.param_fw_diff
        self.param_ow-=lr*self.param_ow_diff
        self.param_pw-=lr*self.param_pw_diff

        self.param_gb-=lr*self.param_gb_diff
        self.param_ib-=lr*self.param_ib_diff
        self.param_fb-=lr*self.param_fb_diff
        self.param_ob-=lr*self.param_ob_diff
        self.param_pb-=lr*self.param_pb_diff


        self.param_gw_diff = np.zeros_like(self.param_gw_diff)
        self.param_iw_diff = np.zeros_like(self.param_iw_diff)
        self.param_fw_diff = np.zeros_like(self.param_fw_diff)
        self.param_ow_diff = np.zeros_like(self.param_ow_diff)
        self.param_pw_diff = np.zeros_like(self.param_pw_diff)

        self.param_gb_diff = np.zeros_like(self.param_gb_diff)
        self.param_ib_diff = np.zeros_like(self.param_ib_diff)
        self.param_fb_diff = np.zeros_like(self.param_fb_diff)
        self.param_ob_diff = np.zeros_like(self.param_ob_diff)
        self.param_pb_diff = np.zeros_like(self.param_pb_diff)

    def lstm_node_add(self,x,y):
        lstm_node=LSTM_node(x,y)
        self.lstm_node_list.append(lstm_node)


class LSTM_node:
    def __init__(self,x,y):

        self.x=x
        self.y=y

        self.state_g=np.zeros(lstm_net.cell_dim)
        self.state_i=np.zeros(lstm_net.cell_dim)
        self.state_f=np.zeros(lstm_net.cell_dim)
        self.state_o=np.zeros(lstm_net.cell_dim)
        self.state_s=np.zeros(lstm_net.cell_dim)
        self.state_h=np.zeros(lstm_net.cell_dim)
        self.state_predict=np.zeros(1)

        self.boottom_dp=np.zeros_like(self.state_predict)
        self.boottom_diff_h=np.zeros_like(self.state_h)
        self.boottom_diff_s=np.zeros_like(self.state_s)
        #LSTM_net.input_dim-LSTM_net.cell_dim==============================x_dim
        self.boottom_diff_x=np.zeros_like(lstm_net.input_dim-lstm_net.cell_dim)




    def run_lstm_node(self,x,pre_s=None,pre_h=None):
        if pre_s==None and pre_h==None:
            pre_s=np.zeros_like(self.state_s)
            pre_h=np.zeros_like(self.state_h)

        self.pre_s=pre_s
        self.pre_h=pre_h

        xc=np.hstack((x,pre_h))

        self.state_g=np.tanh(np.dot(lstm_net.param_gw,xc)+lstm_net.param_gb)
        self.state_i=sigmoid(np.dot(lstm_net.param_iw,xc)+lstm_net.param_ib)
        self.state_f=sigmoid(np.dot(lstm_net.param_fw,xc)+lstm_net.param_fb)
        self.state_o=sigmoid(np.dot(lstm_net.param_ow,xc)+lstm_net.param_ob)

        self.state_s=self.state_g*self.state_i+pre_s*self.state_f
        self.state_h=self.state_s*self.state_o

        # Attention

        self.state_predict=sigmoid(np.dot(lstm_net.param_pw,self.state_h)+lstm_net.param_pb)
        self.x=x
        self.xc=xc

    def diff(self,dp,diff_s,pre_diff_h=None):
        mm=[list(lstm_net.param_pw)]
        bb=np.array(mm).T
        diff_h=np.dot(bb,dp)
        if pre_diff_h!=None:
            diff_h +=  pre_diff_h


        ds=self.state_o*diff_h+diff_s
        do=self.state_s*diff_h
        di=self.state_g*ds
        dg=self.state_i*ds
        df=self.pre_s*ds
###################################################################
        di_input=(1.0-self.state_i)*self.state_i*di
        df_input=(1.0-self.state_f)*self.state_f*df
        do_input=(1.0-self.state_o)*self.state_o*do
        dg_input=(1.0-self.state_g**2)*dg



#####################################################################
        #  xc 通过 if fw ow gw  变换成 i_input,f_input .....
        # 同样 h也通过 p-w 变换成 p
        lstm_net.param_iw_diff+=np.outer(di_input,self.xc)
        lstm_net.param_fw_diff+=np.outer(df_input,self.xc)
        lstm_net.param_ow_diff+=np.outer(do_input,self.xc)
        lstm_net.param_gw_diff+=np.outer(dg_input,self.xc)
        temp=np.outer(dp,self.state_h)
        lstm_net.param_pw_diff+=temp[0]



        lstm_net.param_ib_diff+=di_input
        lstm_net.param_fb_diff+=df_input
        lstm_net.param_ob_diff+=do_input
        lstm_net.param_gb_diff+=dg_input
        lstm_net.param_pb_diff+=dp



        dxc=np.zeros_like(self.xc)
        dxc+=np.dot(lstm_net.param_iw.T,di_input)
        dxc+=np.dot(lstm_net.param_fw.T,df_input)
        dxc+=np.dot(lstm_net.param_ow.T,do_input)
        dxc+=np.dot(lstm_net.param_gw.T,dg_input)

        self.boottom_diff_s=ds*self.state_f
        self.boottom_diff_x=dxc[:x_dim]
        self.boottom_diff_h=dxc[x_dim:]



class LossLayer:
    """
    Computes square loss with first element of hidden layer array.
    """
    @classmethod
    def loss(self, pred, label):
        return (pred - label) ** 2

    @classmethod
    def bottom_diff(self, pred, label):
        diff = np.zeros(1)
        diff[0] = 2 * (pred - label)
        return diff

class Primes:
    def __init__(self):
        self.primes = list()
        for i in range(2, 100):
            is_prime = True
            for j in range(2, i-1):
                if i % j == 0:
                    is_prime = False
            if is_prime:
                self.primes.append(i)
        self.primes_count = len(self.primes)
    def get_sample(self, x_dim, y_dim, index):
        result = np.zeros((x_dim+y_dim))
        for i in range(index, index + x_dim + y_dim):
            result[i-index] = self.primes[i%self.primes_count]/100.0
        return result


def get_data():
    path = "imdb.pkl"
    n_words = 100000
    valid_portion = 0.1
    maxlen = 250
    sort_by_len = True
    f = open(path, 'rb')
    train_set = pickle.load(f)
    test_set = pickle.load(f)
    f.close()
    if maxlen:
        new_train_set_x = []
        new_train_set_y = []
        for x, y in zip(train_set[0], train_set[1]):
            if len(x) < maxlen:

                x.extend([0.001]*(maxlen-len(x)))

                new_train_set_x.append(x)
                new_train_set_y.append(y)
        train_set = (new_train_set_x, new_train_set_y)
        del new_train_set_x, new_train_set_y
    # split training set into validation set
    train_set_x, train_set_y = train_set
    num_samples_total = len(train_set_x)
    sidx = numpy.random.permutation(num_samples_total)
    num_train = int(numpy.round(num_samples_total * (1. - valid_portion)))
    valid_set_x = [train_set_x[s] for s in sidx[num_train:]]
    valid_set_y = [train_set_y[s] for s in sidx[num_train:]]
    train_set_x = [train_set_x[s] for s in sidx[:num_train]]
    train_set_y = [train_set_y[s] for s in sidx[:num_train]]
    train_set = (train_set_x, train_set_y)
    valid_set = (valid_set_x, valid_set_y)

    return train_set,test_set

if __name__ == "__main__":
    x_list=[]
    y_list=[]
    x_dim = 250
    cell_dim = 200
    train_set,test_set=get_data()
    x_list_total=train_set[0]
    y_list_total=train_set[1]

    lstm_net=LSTM_net(x_dim,cell_dim)
    batch = 100
    batch_now = 0
    counter=0
    correct_counter=0
    while batch_now<10000:
        x_list = x_list_total[batch_now:batch + batch_now]
        y_list = y_list_total[batch_now:batch + batch_now]
        batch_now += batch
        for cur_iter in range(100):
            # input x
            for idx in range(0, len(x_list)):
                lstm_net.lstm_node_add(x_list[idx], y_list[idx])
                if idx == 0:
                    lstm_net.lstm_node_list[idx].run_lstm_node(x_list[idx])
                else:
                    lstm_net.lstm_node_list[idx].run_lstm_node(x_list[idx],
                                                               lstm_net.lstm_node_list[idx - 1].state_s,
                                                               lstm_net.lstm_node_list[idx - 1].state_h)
                    # Backpropagation start
            idx = len(x_list) - 1

            counter+=1
            if abs(y_list[idx]-lstm_net.lstm_node_list[idx].state_predict[0])<=0.5:
                correct_counter+=1

            loss = LossLayer.loss(lstm_net.lstm_node_list[idx].state_predict[0], y_list[idx])
            dp = LossLayer.bottom_diff(lstm_net.lstm_node_list[idx].state_predict[0], y_list[idx])
            diff_s = np.zeros(cell_dim)
            lstm_net.lstm_node_list[idx].diff(dp, diff_s)
            idx -= 1

            while idx >= 0:
                counter += 1
                if abs(y_list[idx] - lstm_net.lstm_node_list[idx].state_predict[0]) <= 0.5:
                    correct_counter += 1

                loss += LossLayer.loss(lstm_net.lstm_node_list[idx].state_predict[0], y_list[idx])
                dp = LossLayer.bottom_diff(lstm_net.lstm_node_list[idx].state_predict[0], y_list[idx])
                # dp[0]+=lstm_net.lstm_node_list[idx+1].boottom_dp[0]
                lstm_net.lstm_node_list[idx].diff(dp,
                                                  lstm_net.lstm_node_list[idx + 1].boottom_diff_s,
                                                  lstm_net.lstm_node_list[idx + 1].boottom_diff_h)
                idx -= 1

            if cur_iter % 1000 == 0:
                print "loss: ", loss
                print "accuracy：", correct_counter*1.0/counter

            lstm_net.apply_diff(lr=0.01)
            lstm_net.lstm_node_list = []
            lstm_net.lstm_x_list = []
    print "test start"
    x_list_total = test_set[0]
    y_list_total = test_set[1]
    batch = 10
    batch_now = 0
    counter = 0
    correct_counter = 0
    while batch_now < 10000:
        x_list = x_list_total[batch_now:batch + batch_now]
        y_list = y_list_total[batch_now:batch + batch_now]
        batch_now += batch

        for idx in range(0, len(x_list)):
            lstm_net.lstm_node_add(x_list[idx], y_list[idx])
            if idx == 0:
                lstm_net.lstm_node_list[idx].run_lstm_node(x_list[idx])
            else:
                lstm_net.lstm_node_list[idx].run_lstm_node(x_list[idx],
                                                           lstm_net.lstm_node_list[idx - 1].state_s,
                                                           lstm_net.lstm_node_list[idx - 1].state_h)

        idx = len(x_list) - 1
        loss = 0.0
        while idx >= 0:
            counter += 1
            if abs(y_list[idx] - lstm_net.lstm_node_list[idx].state_predict[0]) <= 0.5:
                correct_counter += 1

            loss += LossLayer.loss(lstm_net.lstm_node_list[idx].state_predict[0], y_list[idx])
            # dp[0]+=lstm_net.lstm_node_list[idx+1].boottom_dp[0]
            idx -= 1

        if batch_now % 100 == 0:
            print "loss: ", loss
            print "accuracy：", correct_counter * 1.0 / counter

        lstm_net.lstm_node_list = []
        lstm_net.lstm_x_list = []
```

[1] Siamese CBOW: Optimizing Word Embedding for Sentence Representations
[2] http://ai.stanford.edu/~amaas/data/sentiment/
[3] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by backpropagating errors. Nature, 323(6088):533–536, 1986.
[5]Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., & Khudanpur, S. (2010). Recurrent neural network based language model. INTERSPEECH 2010, Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September (pp.1045-1048). DBLP.
[6] ATTENTION AND MEMORY IN DEEP LEARNING AND NLP
[7] Le, Q. V., & Mikolov, T. (2014). Distributed representations of sentences and documents. Computer Science, 4, 1188-1196.

**********