
# Deep Learning in NLP

> This is intened as a simple summary for me to review Deep Learning models for NLP task that i learned thesebao days.

> In the end of this post, i constructed a bttention based LSTM to process sentiment analysis as a practice.

> There are still some problems exists, I am working on it now.

# NLP Tasks
Nature Language Processing is about how we process human's language by using coumputer. With the rapid development of hardware (higher arithmetic speed with GPU) and  algoeithms, lots of NLP tasks are going well by implementing Deep Learning models and powerful GPU facilities.
Normally, there are two main compontents in NLP field.

1. Natural Language Understanding (NLU)
	* relation extraction
	* sentiment analysis
	* summarization
	* ...


2. Natural Language Generation (NLG)
	* machine translation
	* Question Answering
	* ...

# Why Deep Learning

Traditional machine learning relies on manual feature extracture while Deep Learning models can extract feature automaticlly, which is extremely convent.
The second reason is that Deep Learning models are powerful.
# Steps in sentiment analysis using Deep Learning model
> Step 1: Map naure language into vectors(Embeddings).

> Step 2: Input model

> Step 3: Train model

> Step 4: Output model

## Embeddings
```flow
st: 起始
register: 注册
condition: 好人?
check: 盘查一下
end: 终了

st > condition
condition(y) > register > end
condition(n) > check > register
```

```flow
st: Word Embeddings
a: Distributed representation
b: One-hot
st>a>b
st>b
```

> Embedding is a core technique in NLP tasks.

When we deal with NLP tasks with Deep Learning models, the first thing we should focus on is to map corpora into embeddings.

### One-hot Representation

A simple method is One-hot Representation.
For example, use One-hot representation to represent "i have a pen"   

|words|representation|
|-----|--------------|
|i|     [1 0 0 0]|
|have|  [0 1 0 0]|
|a|     [0 0 1 0]|
|pen|   [0 0 0 1]|
One-hot representation is easy to understand and corpora processing can be extremely convenient, however, it suffers from two disadvantages.

1. It can be very large in size.
2. It ignores relationships between words.

### Distributed representation
Back to 1986, Rumelhart and Hinton already introduced the earlist use of distrubuted representation, this idea has been proved successful in embedding problem for NLP.
Such approaches are cost-effective and computation-efficient because of low dimension of word embeddings and abundant words relationship.
#### Word to vectors
Basic idea of word2vec is to map words to size fixed vectors (typically 50~200 dimension, which is very short comparing to One-hot repressentatin). Then put all vectors together to construct a word vector space. We can see that each word becaome a point in the vector space which is extremely useful(e.g. the distance between words can be calculated which contains lots of information: near distance means close relationship).





#### Sentence to vectors

word embeddings have proven to be powerful in many NLP tasks, however , are not optimized specilally for representing sentences. In this case, sentence embedding is proposed for two reasons: Word embeddings are usually not task-spcific while sentence embedding can obtain more specific information for different tasks. Anothier reason is that sentence embedding can contain more complete information for representing sentence.

Aimming at optimizing  word  embeddings  for sentence representations. Useful senten embedding method was proposed(Tom Kenter and Alexey Borisov 2016). They present Siamese CBOW, an efficient neural network architecture for obtaining high-quality word embeddings,  directly  optimized  for  sentence representations.
The Siamese CBOW rovides a robust way of generating high-quality sentence.

#### Character to vectors
Generally speaking,English sentence consists of words so approaches toward achieving embeddings are on the word level.
But Japanese and Chinese fundamental units are  Japanese or Chinese character.
Take Chinese as example, Chinese words can be divided into different Chinese character so we can get character embeddings instead of word embedding. Each Chinese character has its own meanning, therefore, model works surprisingly good when use character embeddings as basic input.(e.g. copora data are very small)



## Deep Learning Models
Deep Learning based language models have been proved very powerful  comparing to tradition models such as n-grame model.
NN is effective in NLP.
```sequence
RNN->: LSTM->: Attention Based LSTM
```
### RNNLM--Mikolov
In <Recurrent neural network based language model>, Mikolov use RNN to train language model. The biggest advantage of RNN language model is that it can make good use of information. We can see in Figure 1, RNN can catch history informations. Further more, RNN accepts variable-length input, making NLP tasks easy to deal with.
![Figure 1](../../graph/RNN-overview.png)
<center>RNN-overview.png<center/>
### Attention Based LSTM Language Model
There are some challenges in sentiment analysis, one of them is that sentiment information can appear at any position in the sentence. To tackle these problem, I choose Attention Based LSTM Language Model to deal with a sentimen analysis task.
Given a movie review, the model attempts to predict whether it is positive or negative
## Model
|steps          |operation|
|---------------|---------|
|Input layer    |movie reviews from the Large Movie Review Dataset[2]|
|Embedding layer|map each sentence into a vectors|
|LSTM layer     |capture features from sentence embeddings|
|Attention layer|merge word-level features from each time step|
|Output layer   |merge information from all time step by using Sigmoid from sentimen classification|

 ![Figure 2 AttentionBasedLSTM-Overview](../../graph/AttentionBasedLSTM-Overview.png)
<center> Figure 2 AttentionBasedLSTM_Overview <center>

![Figure 3 AttentionBasedLSTM_detal](../../graph/AttentionBasedLSTM_detal.png)
<center> Figure 3 AttentionBasedLSTM_detal <center>

# Code

```python

```
# problem
the biggest problem i faced is that how to map word into appropiate vector.

[1] Siamese CBOW: Optimizing Word Embeddings for Sentence Representations
[2] http://ai.stanford.edu/~amaas/data/sentiment/
[3] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by backpropagating errors. Nature, 323(6088):533–536, 1986.
