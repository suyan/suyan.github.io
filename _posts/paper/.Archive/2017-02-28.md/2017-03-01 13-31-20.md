[TOC]

# Deep Learning in NLP

> This is intended as a simple summary for me to review Deep Learning models for NLP task that I learned these days.
> In the end of this post, I constructed an attention based LSTM to process sentiment analysis as a practice.
> There are still some problems exists, I am working on it now.

## My Interests in NLP
1. Is embedding good enough to represent semantic? can we find other approaches? 
2. Does RNN includes biological meanings? Can we construct biology neural system based algorithom for NLP just like convolution for image processing?
3. Train data are  small in size, furthermore, data form is too simple.
4. Unsupervised learning is a trend of machine learning, especially in NLP field.  In most case, NLP tasks have no specific answer, it will be better to let models learn by themself. It can also relieve the data pressure.
## NLP Tasks

Normally, there are two main components in NLP field.

1. Natural Language Understanding (NLU)
	* relation extraction
	* sentiment analysis
	* summarization
	* ...


2. Natural Language Generation (NLG)
	* machine translation
	* Question Answering
	* ...

## Why Deep Learning
Nature Language Processing is about teaching computer to process human language. Since high peformance hardware and powerful deep learning algorithms are available, lots of NLP tasks are going well by implementing Deep Learning models.
Traditional machine learning relies on manual feature extraction while Deep Learning models can extract feature automatically, which is extremely convent.
The second reason is that Deep Learning models are powerful.
# Steps in sentiment analysis using Deep Learning model
> Step 1: Map nature language into vectors(Embedding).

> Step 2: Input model

> Step 3: Train model

> Step 4: Output model

## Embedding
> Embedding is a core technique in NLP tasks.
```flow
Word Embedding->: Distributed Representation
Word Embedding->: One-hot
```


When we deal with NLP tasks with Deep Learning models, the first thing we should focus on is to map corpora into embedding.

### One-hot Representation

A simple method is One-hot Representation.
For example, use One-hot representation to represent "I have a pen"   

|words | representation|
|----- |--------------|
|I|     [1 0 0 0]|
|have|  [0 1 0 0]|
|a|     [0 0 1 0]|
|pen|   [0 0 0 1]|
One-hot representation is easy to understand and corpora processing can be extremely convenient, however, it suffers from two disadvantages.

1. It can be very large in size.
2. It ignores relationships between words.

### Distributed representation
Back to 1986, Rumelhart and Hinton already introduced the earliest use of distributed representation, this idea has been proved successful in embedding problem for NLP.
Such approaches are cost-effective and computation-efficient because of low dimension of word embedding and abundant words relationship.
#### Word to vectors
Basic idea of word2vec is to map words to size fixed vectors (typically 50~200 dimension, which is very short comparing to One-hot representatin). Then put all vectors together to construct a word vector space. We can see that each word becomes a point in the vector space which is extremely useful(e.g. the distance between words can be calculated which contains lots of information: near distance means close relationship).





#### Sentence to vectors

word embedding have proven to be powerful in many NLP tasks, however , are not optimized specialaly for representing sentences. In this case, sentence embedding is proposed for two reasons: Word embedding are usually not task-specific while sentence embedding can obtain more specific information for different tasks. Another reason is that sentence embedding can contain more complete information for representing sentence.

Aiming at optimizing  word  embedding  for sentence representations. Useful sentence embedding method was proposed(Tom Kenter and Alexey Borisov 2016). They present Siamese CBOW, an efficient neural network architecture for obtaining high-quality word embedding,  directly  optimized  for  sentence representations.
The Siamese CBOW provides a robust way of generating high-quality sentence.

#### Character to vectors
Generally speaking, English sentence consists of words so approaches toward achieving embedding are on the word level.
But Japanese and Chinese fundamental units are  Japanese or Chinese character.
Take Chinese as example, Chinese words can be divided into different Chinese character so we can get character embedding instead of word embedding. Each Chinese character has its own meaning, therefore, model works surprisingly good when use character embedding as basic input.(e.g. corpora data are very small)



## Deep Learning Models
Deep Learning based language models have been proved very powerful  comparing to tradition models such as n-gram model.
NN is effective in NLP.
```sequence
RNN->: LSTM->: Attention Based LSTM
```
### RNNLM--Mikolov
In <Recurrent neural network based language model>, Mikolov use RNN to train language model. The biggest advantage of RNN language model is that it can make good use of information. We can see in Figure 1, RNN can catch history information. Further more, RNN accepts variable-length input, making NLP tasks easy to deal with.
![Figure 1](../../graph/RNN-overview.png)
<center>RNN-overview.png<center/>
### Attention Based LSTM Language Model
There are some challenges in sentiment analysis, one of them is that sentiment information can appear at any position in the sentence. To tackle these problem, I choose Attention Based LSTM Language Model to deal with a sentiment analysis task.
Given a movie review, the model attempts to predict whether it is positive or negative
## Model
|steps          |operation|
|---------------|---------|
|Input layer    |movie reviews from the Large Movie Review Dataset[2]|
|Embedding layer |map each sentence into a vectors|
|LSTM layer     |capture features from sentence embedding|
|Attention layer |merge word-level features from each time step|
|Output layer   |merge information from all time step by using Sigmoid from sentiment classification|

 ![Figure 2 AttentionBasedLSTM-Overview](../../graph/AttentionBasedLSTM-Overview.png)
<center> Figure 2 AttentionBasedLSTM_Overview <center>

![Figure 3 AttentionBasedLSTM_detal](../../graph/AttentionBasedLSTM_detal.png)
<center> Figure 3 AttentionBasedLSTM_detal <center>

# Code

```python

```

[1] Siamese CBOW: Optimizing Word Embedding for Sentence Representations
[2] http://ai.stanford.edu/~amaas/data/sentiment/
[3] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by backpropagating errors. Nature, 323(6088):533â€“536, 1986.
